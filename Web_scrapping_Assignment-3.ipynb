{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.1) Write a python program which searches all the product under a particular product from www.amazon.in.The product to be searched will be taken as input from user.\n",
    "\n",
    "## Q.2) And then scrapping the following details of each product listed in first 3 pages of  search results and saving it in a data frame and csv. In case if any product has less than 3 pages in search results then we can scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Expected Delivery\", \"Availability\" and\"Product URL\". In case, if any of the details are missing for any of the product then replacing it by \"-\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impoerting required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to a webdriver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching an web element for search bar using xpath\n",
    "search_bar = driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guitar\n"
     ]
    }
   ],
   "source": [
    "#Now,taking the User input to search the particular product\n",
    "search_bar.send_keys(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for search button,we can get a web element for that using an absolute xpath\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "\n",
    "#Searching for a product\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the page_source\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list,brand,name,price_t,avai,exp_del,ret_exc = [],[],[],[],[],[],[]\n",
    "\n",
    "\n",
    "#finding all the items in page\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "result = soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "    \n",
    "#finding web element of item by class\n",
    "for e_item in range(len(result)):\n",
    "    item = result[e_item]\n",
    "    atag = item.h2.a\n",
    "    #     brand.append((atag.text.strip().split())[0])\n",
    "    name.append(atag.text.strip())\n",
    "    try:\n",
    "        p_price = item.find('span','a-price')\n",
    "        price = p_price.find('span','a-offscreen').text.strip()\n",
    "        price_t.append(price)\n",
    "\n",
    "    except:\n",
    "        price_t.append('-')\n",
    "\n",
    "\n",
    "    #getting an url of particular product\n",
    "    url = 'https://www.amazon.in' + atag.get('href')\n",
    "    url_list.append(url)\n",
    "    driver.get(url)\n",
    "\n",
    "    soup_n = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "    try:\n",
    "        result_n = soup_n.find('div',{'id':'availability'})\n",
    "        avai.append(result_n.text.strip())\n",
    "\n",
    "    except:\n",
    "        avai.append('-')\n",
    "\n",
    "    try:\n",
    "        result_n = soup_n.find('div',{'id':'ddmDeliveryMessage'})\n",
    "        exp_del.append(result_n.b.text.strip())\n",
    "\n",
    "\n",
    "    except:\n",
    "        exp_del.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        brand_name = soup_n.find('td',{'class':'a-span9'})\n",
    "        brand.append(brand_name.text.strip())\n",
    "\n",
    "    except:\n",
    "        brand.append(\"-\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        rep_re = soup_n.find('div',{'id':'RETURNS_POLICY'})\n",
    "        ret_exc.append(rep_re.text.strip())\n",
    "        driver.back()\n",
    "    except:\n",
    "        ret_exc.append(\"-\")\n",
    "        driver.back()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2,page3 = False,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    next_but = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[64]/span/div/span/a[1]\")\n",
    "    next_but.click()\n",
    "    page2 = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for page 2\n",
    "\n",
    "if page2 == True:\n",
    "    #finding all the items in page\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    result = soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "\n",
    "    #finding web element of item by class\n",
    "    for e_item in range(len(result)):\n",
    "        item = result[e_item]\n",
    "        atag = item.h2.a\n",
    "        #     brand.append((atag.text.strip().split())[0])\n",
    "        name.append(atag.text.strip())\n",
    "        try:\n",
    "            p_price = item.find('span','a-price')\n",
    "            price = p_price.find('span','a-offscreen').text.strip()\n",
    "            price_t.append(price)\n",
    "\n",
    "        except:\n",
    "            price_t.append('-')\n",
    "\n",
    "\n",
    "        #getting an url of particular product\n",
    "        url = 'https://www.amazon.in' + atag.get('href')\n",
    "        url_list.append(url)\n",
    "        driver.get(url)\n",
    "\n",
    "        soup_n = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            result_n = soup_n.find('div',{'id':'availability'})\n",
    "            avai.append(result_n.text.strip())\n",
    "\n",
    "        except:\n",
    "            avai.append('-')\n",
    "\n",
    "        try:\n",
    "            result_n = soup_n.find('div',{'id':'ddmDeliveryMessage'})\n",
    "            exp_del.append(result_n.b.text.strip())\n",
    "        except:\n",
    "            exp_del.append(\"-\") \n",
    "        try:\n",
    "            brand_name = soup_n.find('td',{'class':'a-span9'})\n",
    "            brand.append(brand_name.text.strip())\n",
    "\n",
    "        except:\n",
    "            brand.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            rep_re = soup_n.find('div',{'id':'RETURNS_POLICY'})\n",
    "            ret_exc.append(rep_re.text.strip())\n",
    "            driver.back()\n",
    "        except:\n",
    "            ret_exc.append(\"-\")\n",
    "            driver.back()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    next_but = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/span[3]/div[2]/div[52]/span/div/span/a[3]\")\n",
    "    next_but.click()\n",
    "    page3 = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for page 3\n",
    "\n",
    "if page3 == True:\n",
    "    #finding all the items in page\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    result = soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "\n",
    "    #finding web element of item by class\n",
    "    for e_item in range(len(result)):\n",
    "        item = result[e_item]\n",
    "        atag = item.h2.a\n",
    "        #     brand.append((atag.text.strip().split())[0])\n",
    "        name.append(atag.text.strip())\n",
    "        try:\n",
    "            p_price = item.find('span','a-price')\n",
    "            price = p_price.find('span','a-offscreen').text.strip()\n",
    "            price_t.append(price)\n",
    "\n",
    "        except:\n",
    "            price_t.append('-')\n",
    "\n",
    "\n",
    "        #getting an url of particular product\n",
    "        url = 'https://www.amazon.in' + atag.get('href')\n",
    "        url_list.append(url)\n",
    "        driver.get(url)\n",
    "\n",
    "        soup_n = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            result_n = soup_n.find('div',{'id':'availability'})\n",
    "            avai.append(result_n.text.strip())\n",
    "\n",
    "        except:\n",
    "            avai.append('-')\n",
    "\n",
    "        try:\n",
    "            result_n = soup_n.find('div',{'id':'ddmDeliveryMessage'})\n",
    "            exp_del.append(result_n.b.text.strip())\n",
    "\n",
    "\n",
    "        except:\n",
    "            exp_del.append(\"-\")\n",
    "\n",
    "    \n",
    "        try:\n",
    "            brand_name = soup_n.find('td',{'class':'a-span9'})\n",
    "            brand.append(brand_name.text.strip())\n",
    "\n",
    "        except:\n",
    "            brand.append(\"-\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            rep_re = soup_n.find('div',{'id':'RETURNS_POLICY'})\n",
    "            ret_exc.append(rep_re.text.strip())\n",
    "            driver.back()\n",
    "        except:\n",
    "            ret_exc.append(\"-\")\n",
    "            driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Name of the Product</th>\n",
       "      <th>Price</th>\n",
       "      <th>Return/Exchange</th>\n",
       "      <th>Expected Delivery</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Frontier guitar with Online Guitar lea...</td>\n",
       "      <td>₹4,999</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Monday, Jan 31</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Frontier Jumbo Semi Acoustic Guitar Wi...</td>\n",
       "      <td>₹6,499</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Monday, Jan 31</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Guitar Acoustica Series, 41” Jumbo Siz...</td>\n",
       "      <td>₹6,799</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Saturday, Jan 29</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multicolor</td>\n",
       "      <td>JOY&amp;TOY Enterprise 4-String Acoustic Guitar Le...</td>\n",
       "      <td>₹989</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Feb 3 - 9</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUAREZ</td>\n",
       "      <td>Juarez Acoustic Guitar, 38 Inch Curved Body Cu...</td>\n",
       "      <td>₹2,190</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Sunday, Jan 30</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/Juarez-Acoustic-Guitar-C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Brand Name                                Name of the Product   Price  \\\n",
       "0     Kadence  Kadence Frontier guitar with Online Guitar lea...  ₹4,999   \n",
       "1     Kadence  Kadence Frontier Jumbo Semi Acoustic Guitar Wi...  ₹6,499   \n",
       "2     Kadence  Kadence Guitar Acoustica Series, 41” Jumbo Siz...  ₹6,799   \n",
       "3  Multicolor  JOY&TOY Enterprise 4-String Acoustic Guitar Le...    ₹989   \n",
       "4      JUAREZ  Juarez Acoustic Guitar, 38 Inch Curved Body Cu...  ₹2,190   \n",
       "\n",
       "      Return/Exchange Expected Delivery Availability  \\\n",
       "0  7 Days Replacement    Monday, Jan 31    In stock.   \n",
       "1  7 Days Replacement    Monday, Jan 31    In stock.   \n",
       "2  7 Days Replacement  Saturday, Jan 29    In stock.   \n",
       "3  7 Days Replacement         Feb 3 - 9    In stock.   \n",
       "4  7 Days Replacement    Sunday, Jan 30    In stock.   \n",
       "\n",
       "                                         Product URL  \n",
       "0  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "1  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "2  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "3  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "4  https://www.amazon.in/Juarez-Acoustic-Guitar-C...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Brand Name\":brand,\"Name of the Product\":name,\"Price\":price_t,\"Return/Exchange\":ret_exc,\n",
    "                  \"Expected Delivery\":exp_del,\"Availability\":avai,\"Product URL\":url_list})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('amazon_products.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.4) Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching an web element for search bar using xpath\n",
    "search_bar = driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel 4A\n"
     ]
    }
   ],
   "source": [
    "#Now,taking the User input to search the particular product\n",
    "search_bar.send_keys(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for search button,we can get a web element for that using an absolute xpath\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "\n",
    "#Searching for a product\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding all the items in page\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "result = soup.find_all('a',{'class':'_1fQZEK'})\n",
    "\n",
    "#for price\n",
    "price = []\n",
    "for i in result:\n",
    "    a = i.find('div',{\"class\":\"_30jeq3 _1_WHN1\"})\n",
    "    price.append(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating all requrired lists\n",
    "b_name,smart_p,colour,RAM,ROM,p_cam,s_cam,Disp,Battery,prod_url = [],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "for e_item in range(len(result)):\n",
    "    item = result[e_item]\n",
    "    \n",
    "    #for product url\n",
    "    prod_url.append(\"https://www.flipkart.com\" + item.get('href'))\n",
    "    \n",
    "a = soup.find_all('div',{'class':'_4rR01T'})\n",
    "smart_p1 = [i.text.strip() for i in a]\n",
    "\n",
    "#for brand name\n",
    "b_name = [(i.split(\" \"))[0] for i in smart_p1 ]\n",
    "\n",
    "#for smartphone name\n",
    "smart_p = [(i.split(\"(\"))[0].strip() for i in smart_p1 ]\n",
    "\n",
    "#for color\n",
    "for i in smart_p1:\n",
    "    try:\n",
    "        i = (((i.split(\"(\"))[1]).split(\",\"))[0]\n",
    "        colour.append(i)\n",
    "    except:\n",
    "        colour.append(\"-\")\n",
    "\n",
    "#for RAM and ROM\n",
    "a = soup.find_all('div',{'class':'fMghEO'})\n",
    "\n",
    "for i in a:\n",
    "    b = i.find('li',{'class':'rgWa7D'})\n",
    "    ROM.append((b.text.split(\"|\"))[1].strip())\n",
    "    RAM.append((b.text.split(\"|\"))[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_new = driver.find_elements_by_xpath('//ul[@class=\"_1xgFaf\"]')\n",
    "\n",
    "#for display size\n",
    "Disp = [((i.text).split(\"\\n\"))[1]  if \"cm\" in ((i.text).split(\"\\n\"))[1] else '-'for i in b_new]\n",
    "\n",
    "#for primary and secondary camera\n",
    "p_cam = [(((i.text).split(\"\\n\"))[2].split(\"|\"))[0] for i in b_new]\n",
    "s_cam = [(((i.text).split(\"\\n\"))[2].split(\"|\"))[1] if \"|\" in ((i.text).split(\"\\n\"))[2] else '-' for i in b_new]\n",
    "\n",
    "#for battery\n",
    "battery = [((i.text).split(\"\\n\"))[3] if \"Battery\" in ((i.text).split(\"\\n\"))[3] else '-' for i in b_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Smartphone name</th>\n",
       "      <th>Colour</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Storage(ROM)</th>\n",
       "      <th>Primary Camera</th>\n",
       "      <th>Secondary Camera</th>\n",
       "      <th>Display Size</th>\n",
       "      <th>Battery Capacity</th>\n",
       "      <th>Price</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>Google Pixel 4a</td>\n",
       "      <td>Just Black</td>\n",
       "      <td>6 GB RAM</td>\n",
       "      <td>128 GB ROM</td>\n",
       "      <td>12.2MP Rear Camera</td>\n",
       "      <td>8MP Front Camera</td>\n",
       "      <td>14.76 cm (5.81 inch) Full HD+ Display</td>\n",
       "      <td>3140 mAh Battery</td>\n",
       "      <td>₹27,999</td>\n",
       "      <td>https://www.flipkart.com/google-pixel-4a-just-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAMSUNG</td>\n",
       "      <td>SAMSUNG Galaxy M52 5G</td>\n",
       "      <td>Blazing Black</td>\n",
       "      <td>6 GB RAM</td>\n",
       "      <td>128 GB ROM</td>\n",
       "      <td>64MP + 12MP + 5MP</td>\n",
       "      <td>32MP Front Camera</td>\n",
       "      <td>17.02 cm (6.7 inch) Full HD+ Display</td>\n",
       "      <td>5000 mAh Battery</td>\n",
       "      <td>₹26,995</td>\n",
       "      <td>https://www.flipkart.com/samsung-galaxy-m52-5g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Redmi</td>\n",
       "      <td>Redmi 9A Sport</td>\n",
       "      <td>Carbon Black</td>\n",
       "      <td>2 GB RAM</td>\n",
       "      <td>32 GB ROM</td>\n",
       "      <td>13MP Rear Camera</td>\n",
       "      <td>5MP Front Camera</td>\n",
       "      <td>16.59 cm (6.53 inch) HD+ Display</td>\n",
       "      <td>5000 mAh Battery</td>\n",
       "      <td>₹7,749</td>\n",
       "      <td>https://www.flipkart.com/redmi-9a-sport-carbon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Redmi</td>\n",
       "      <td>Redmi 9A</td>\n",
       "      <td>SeaBlue</td>\n",
       "      <td>2 GB RAM</td>\n",
       "      <td>32 GB ROM</td>\n",
       "      <td>13MP Rear Camera</td>\n",
       "      <td>-</td>\n",
       "      <td>16.59 cm (6.53 inch) Full HD+ Display</td>\n",
       "      <td>5000 mAh Battery</td>\n",
       "      <td>₹7,699</td>\n",
       "      <td>https://www.flipkart.com/redmi-9a-seablue-32-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Redmi</td>\n",
       "      <td>Redmi 9A</td>\n",
       "      <td>Midnight Black</td>\n",
       "      <td>2 GB RAM</td>\n",
       "      <td>32 GB ROM</td>\n",
       "      <td>13MP Rear Camera</td>\n",
       "      <td>-</td>\n",
       "      <td>16.59 cm (6.53 inch) Full HD+ Display</td>\n",
       "      <td>5000 mAh Battery</td>\n",
       "      <td>₹7,724</td>\n",
       "      <td>https://www.flipkart.com/redmi-9a-midnight-bla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Brand Name        Smartphone name          Colour       RAM Storage(ROM)  \\\n",
       "0     Google        Google Pixel 4a      Just Black  6 GB RAM   128 GB ROM   \n",
       "1    SAMSUNG  SAMSUNG Galaxy M52 5G   Blazing Black  6 GB RAM   128 GB ROM   \n",
       "2      Redmi         Redmi 9A Sport    Carbon Black  2 GB RAM    32 GB ROM   \n",
       "3      Redmi               Redmi 9A         SeaBlue  2 GB RAM    32 GB ROM   \n",
       "4      Redmi               Redmi 9A  Midnight Black  2 GB RAM    32 GB ROM   \n",
       "\n",
       "        Primary Camera    Secondary Camera  \\\n",
       "0  12.2MP Rear Camera     8MP Front Camera   \n",
       "1   64MP + 12MP + 5MP    32MP Front Camera   \n",
       "2    13MP Rear Camera     5MP Front Camera   \n",
       "3     13MP Rear Camera                   -   \n",
       "4     13MP Rear Camera                   -   \n",
       "\n",
       "                            Display Size  Battery Capacity    Price  \\\n",
       "0  14.76 cm (5.81 inch) Full HD+ Display  3140 mAh Battery  ₹27,999   \n",
       "1   17.02 cm (6.7 inch) Full HD+ Display  5000 mAh Battery  ₹26,995   \n",
       "2       16.59 cm (6.53 inch) HD+ Display  5000 mAh Battery   ₹7,749   \n",
       "3  16.59 cm (6.53 inch) Full HD+ Display  5000 mAh Battery   ₹7,699   \n",
       "4  16.59 cm (6.53 inch) Full HD+ Display  5000 mAh Battery   ₹7,724   \n",
       "\n",
       "                                         Product URL  \n",
       "0  https://www.flipkart.com/google-pixel-4a-just-...  \n",
       "1  https://www.flipkart.com/samsung-galaxy-m52-5g...  \n",
       "2  https://www.flipkart.com/redmi-9a-sport-carbon...  \n",
       "3  https://www.flipkart.com/redmi-9a-seablue-32-g...  \n",
       "4  https://www.flipkart.com/redmi-9a-midnight-bla...  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Brand Name\":b_name,\"Smartphone name\":smart_p,\"Colour\":colour,\"RAM\":RAM,\n",
    "                  \"Storage(ROM)\":ROM,\"Primary Camera\":p_cam,\"Secondary Camera\":s_cam,\"Display Size\":Disp,\n",
    "                  \"Battery Capacity\":battery,\"Price\":price,\"Product URL\":prod_url})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('flipkart_smartphones.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.6) Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://trak.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching an web element for more options using xpath\n",
    "search_bar = driver.find_element_by_xpath('/html/body/div[1]/header/div[2]/div/div/div/div/nav/ul/li[8]/a')\n",
    "\n",
    "search_bar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching an web element for funding deals options using xpath\n",
    "search_bar = driver.find_element_by_xpath('/html/body/div[1]/header/div[2]/div/div/div/div/nav/ul/li[8]/ul/li[2]/a')\n",
    "\n",
    "search_bar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating all required lists\n",
    "date,s_name,indust,s_vertical,c_loc,i_name,inv_type,amount = [],[],[],[],[],[],[],[]\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "result = soup.find_all('tbody',{'class':'row-hover'})\n",
    "\n",
    "#extracting data only for second quarter (i.e Jan 21 – March 21) \n",
    "result = [i for i in result][1:4]\n",
    "\n",
    "for i in range(len(result)):\n",
    "    \n",
    "    #for extractng the date\n",
    "    date_n = result[i].find_all('td',{'class':'column-2'})\n",
    "    for d_i in date_n:\n",
    "        date.append(d_i.text)\n",
    "        \n",
    "    #for extracting the startup name\n",
    "    name = result[i].find_all('td',{'class':'column-3'})\n",
    "    for d_i in name:\n",
    "        s_name.append(d_i.text)\n",
    "    \n",
    "    #for industry\n",
    "    industry = result[i].find_all('td',{'class':'column-4'})\n",
    "    for d_i in industry:\n",
    "        indust.append(d_i.text)\n",
    "    \n",
    "    #for sub vertical\n",
    "    sub_vertical = result[i].find_all('td',{'class':'column-5'})\n",
    "    for d_i in sub_vertical:\n",
    "        s_vertical.append(d_i.text)\n",
    "    \n",
    "    #for city location\n",
    "    city_location = result[i].find_all('td',{'class':'column-6'})\n",
    "    for d_i in city_location:\n",
    "        c_loc.append(d_i.text)\n",
    "    \n",
    "    #for investor name\n",
    "    invest_name = result[i].find_all('td',{'class':'column-7'})\n",
    "    for d_i in invest_name:\n",
    "        i_name.append(d_i.text)\n",
    "    \n",
    "    #for investment type\n",
    "    invest_type = result[i].find_all('td',{'class':'column-8'})\n",
    "    for d_i in invest_type:\n",
    "        inv_type.append(d_i.text)\n",
    "    \n",
    "    #for Amount\n",
    "    Amount = result[i].find_all('td',{'class':'column-9'})\n",
    "    for d_i in Amount:\n",
    "        amount.append(d_i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date(dd\\mm\\yy)</th>\n",
       "      <th>Startup Name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>sub-vertical</th>\n",
       "      <th>City\\Location</th>\n",
       "      <th>Investor's name</th>\n",
       "      <th>Investment type</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04/03/2021</td>\n",
       "      <td>DealShare</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online shopping platform</td>\n",
       "      <td>Jaipur, Rajasthan</td>\n",
       "      <td>Innoven Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31/03/2021</td>\n",
       "      <td>Uniphore</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Conversational Service Automation (CSA)</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Sorenson Capital Partners</td>\n",
       "      <td>Series D</td>\n",
       "      <td>140,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Hyper-local delivery app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Krishtal Advisors Pte Ltd</td>\n",
       "      <td>Series E</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>BYJU’S</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Online tutoring</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>MC Global Edtech, B Capital, Baron, others</td>\n",
       "      <td>Series F</td>\n",
       "      <td>460,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23/03/2021</td>\n",
       "      <td>SkilloVilla</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Career and job-oriented upskilling.</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Titan Capital, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>300,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25/03/2021</td>\n",
       "      <td>CityMall</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Social ecommerce and online grocery platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Accel Partners</td>\n",
       "      <td>Series A</td>\n",
       "      <td>11,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26/03/2021</td>\n",
       "      <td>DotPe</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Commerce and payments platform to offline ente...</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Series A</td>\n",
       "      <td>27,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11/02/2021</td>\n",
       "      <td>Doubtnut</td>\n",
       "      <td>Edu Tech</td>\n",
       "      <td>E-Learning Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SIG Global, Sequoia Capital, WaterBridge Ventu...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>2,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22/02/2021</td>\n",
       "      <td>Zomato</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Online Food Delivery Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Tiger Global, Kora</td>\n",
       "      <td>Venture</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19/02/2021</td>\n",
       "      <td>Fingerlix</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Semi-cooked food delivery app</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Rhodium Trust, Accel Partners and Swiggy</td>\n",
       "      <td>Series C</td>\n",
       "      <td>2,747,045.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17/02/2021</td>\n",
       "      <td>Zolve</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Global Neobank Venture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Accel Partners and Lightspeed Venture Partners</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,50,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15/02/2021</td>\n",
       "      <td>KreditBee</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Digital lending platform</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Azim Premji’s PremjiInvest and South Korea’s M...</td>\n",
       "      <td>Series C</td>\n",
       "      <td>75,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Pepperfry</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Multi-brand furniture brand</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>InnoVen Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>4,773,958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Online supermarket</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SoftBank Vision Fund (SVF)</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>55,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Technology Venture</td>\n",
       "      <td>London</td>\n",
       "      <td>GV</td>\n",
       "      <td>Series A</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>SplashLearn</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Game-based learning programme</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Owl Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>18,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15/01/2021</td>\n",
       "      <td>Digit Insurance</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Insurance Services</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>A91 Partners, Faering Capital, TVS Capital Funds</td>\n",
       "      <td>Venture</td>\n",
       "      <td>1,80,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28/01/2021</td>\n",
       "      <td>Bombay Shaving Company</td>\n",
       "      <td>Consumer Goods Company</td>\n",
       "      <td>Shave care, beard care, and skincare products</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Reckitt Benckiser</td>\n",
       "      <td>Venture</td>\n",
       "      <td>6,172,258.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>DeHaat</td>\n",
       "      <td>AgriTech Startup</td>\n",
       "      <td>online marketplace for farm products and services</td>\n",
       "      <td>Patna</td>\n",
       "      <td>Prosus Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>Darwinbox</td>\n",
       "      <td>SaaS</td>\n",
       "      <td>HR Tech</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Salesforce Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>mfine</td>\n",
       "      <td>Health Tech Startup</td>\n",
       "      <td>AI-powered telemedicine mobile app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Heritas Capital Management</td>\n",
       "      <td>Venture Round</td>\n",
       "      <td>16,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>Udayy</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Online learning platform for kids in class 1-5</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Sequoia Capital</td>\n",
       "      <td>Seed Funding</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11/01/2021</td>\n",
       "      <td>True Elements</td>\n",
       "      <td>Food Startup</td>\n",
       "      <td>Whole Food plant based Nashta</td>\n",
       "      <td>Pune</td>\n",
       "      <td>SIDBI Venture Capital</td>\n",
       "      <td>Series</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13/01/2021</td>\n",
       "      <td>Saveo</td>\n",
       "      <td>B2B E-commerce</td>\n",
       "      <td>Pharmacies</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Matrix Partners India, RTP Global, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>4,000,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date(dd\\mm\\yy)            Startup Name                Industry  \\\n",
       "0      04/03/2021               DealShare              E-commerce   \n",
       "1      31/03/2021                Uniphore              Technology   \n",
       "2      30/03/2021                   Dunzo              E-commerce   \n",
       "3      30/03/2021                  BYJU’S                Edu-tech   \n",
       "4      23/03/2021             SkilloVilla                Edu-tech   \n",
       "5      25/03/2021                CityMall              E-commerce   \n",
       "6      26/03/2021                   DotPe                 FinTech   \n",
       "7      11/02/2021                Doubtnut                Edu Tech   \n",
       "8      22/02/2021                  Zomato             Hospitality   \n",
       "9      19/02/2021               Fingerlix             Hospitality   \n",
       "10     17/02/2021                   Zolve                 FinTech   \n",
       "11     15/02/2021               KreditBee                 Finance   \n",
       "12     12/02/2021               Pepperfry              E-commerce   \n",
       "13     12/02/2021                 Grofers              E-Commerce   \n",
       "14     09/02/2021                 Nothing              Technology   \n",
       "15     09/02/2021             SplashLearn                  EdTech   \n",
       "16     15/01/2021         Digit Insurance      Financial Services   \n",
       "17     28/01/2021  Bombay Shaving Company  Consumer Goods Company   \n",
       "18     19/01/2021                  DeHaat        AgriTech Startup   \n",
       "19     19/01/2021               Darwinbox                    SaaS   \n",
       "20     18/01/2021                   mfine     Health Tech Startup   \n",
       "21     18/01/2021                   Udayy                  EdTech   \n",
       "22     11/01/2021           True Elements            Food Startup   \n",
       "23     13/01/2021                   Saveo          B2B E-commerce   \n",
       "\n",
       "                                         sub-vertical      City\\Location  \\\n",
       "0                            Online shopping platform  Jaipur, Rajasthan   \n",
       "1             Conversational Service Automation (CSA)          Palo Alto   \n",
       "2                            Hyper-local delivery app          Bengaluru   \n",
       "3                                     Online tutoring          Bengaluru   \n",
       "4                 Career and job-oriented upskilling.          Bengaluru   \n",
       "5        Social ecommerce and online grocery platform            Gurgaon   \n",
       "6   Commerce and payments platform to offline ente...            Gurgaon   \n",
       "7                                 E-Learning Platform            Gurgaon   \n",
       "8                       Online Food Delivery Platform            Gurgaon   \n",
       "9                       Semi-cooked food delivery app             Mumbai   \n",
       "10                             Global Neobank Venture             Mumbai   \n",
       "11                           Digital lending platform          Bengaluru   \n",
       "12                        Multi-brand furniture brand             Mumbai   \n",
       "13                                 Online supermarket            Gurgaon   \n",
       "14                        Consumer Technology Venture             London   \n",
       "15                      Game-based learning programme            Gurgaon   \n",
       "16                                 Insurance Services          Bengaluru   \n",
       "17      Shave care, beard care, and skincare products          New Delhi   \n",
       "18  online marketplace for farm products and services              Patna   \n",
       "19                                            HR Tech             Mumbai   \n",
       "20                 AI-powered telemedicine mobile app          Bengaluru   \n",
       "21     Online learning platform for kids in class 1-5            Gurgaon   \n",
       "22                      Whole Food plant based Nashta               Pune   \n",
       "23                                         Pharmacies          Bengaluru   \n",
       "\n",
       "                                      Investor's name Investment type  \\\n",
       "0                                     Innoven Capital  Debt Financing   \n",
       "1                           Sorenson Capital Partners        Series D   \n",
       "2                           Krishtal Advisors Pte Ltd        Series E   \n",
       "3          MC Global Edtech, B Capital, Baron, others        Series F   \n",
       "4                               Titan Capital, others            Seed   \n",
       "5                                      Accel Partners        Series A   \n",
       "6                                                PayU        Series A   \n",
       "7   SIG Global, Sequoia Capital, WaterBridge Ventu...        Series B   \n",
       "8                                  Tiger Global, Kora         Venture   \n",
       "9            Rhodium Trust, Accel Partners and Swiggy        Series C   \n",
       "10     Accel Partners and Lightspeed Venture Partners            Seed   \n",
       "11  Azim Premji’s PremjiInvest and South Korea’s M...        Series C   \n",
       "12                                    InnoVen Capital  Debt Financing   \n",
       "13                         SoftBank Vision Fund (SVF)     Unspecified   \n",
       "14                                                 GV        Series A   \n",
       "15                                       Owl Ventures        Series C   \n",
       "16   A91 Partners, Faering Capital, TVS Capital Funds         Venture   \n",
       "17                                  Reckitt Benckiser         Venture   \n",
       "18                                    Prosus Ventures        Series C   \n",
       "19                                Salesforce Ventures            Seed   \n",
       "20                         Heritas Capital Management   Venture Round   \n",
       "21                                   Sequoia Capital     Seed Funding   \n",
       "22                              SIDBI Venture Capital          Series   \n",
       "23          Matrix Partners India, RTP Global, others            Seed   \n",
       "\n",
       "          Amount  \n",
       "0    250,000,000  \n",
       "1    140,000,000  \n",
       "2      8,000,000  \n",
       "3    460,000,000  \n",
       "4    300,000,000  \n",
       "5     11,000,000  \n",
       "6     27,500,000  \n",
       "7      2,500,000  \n",
       "8    250,000,000  \n",
       "9   2,747,045.20  \n",
       "10   1,50,00,000  \n",
       "11    75,000,000  \n",
       "12     4,773,958  \n",
       "13    55,000,000  \n",
       "14    15,000,000  \n",
       "15    18,000,000  \n",
       "16  1,80,00,000   \n",
       "17  6,172,258.50  \n",
       "18    30,000,000  \n",
       "19    15,000,000  \n",
       "20    16,000,000  \n",
       "21    15,000,000  \n",
       "22   100,000,000  \n",
       "23    4,000,000   "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Date(dd\\mm\\yy)\":date,\"Startup Name\":s_name,\"Industry\":indust,\"sub-vertical\":s_vertical,\n",
    "                  \"City\\Location\":c_loc,\"Investor's name\":i_name,\"Investment type\":inv_type,\"Amount\":amount})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Funding deals of second quarter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.7) Write a program to scrap all the available details of  best gaming laptops(Top 10) from https://www.digit.in/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://www.digit.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searchng for a web element of best gaming laptops\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[2]/div[2]/div[4]/ul/li[9]/a\")\n",
    "\n",
    "#to click on selected option\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the laptops name\n",
    "l_name = driver.find_elements_by_xpath('//div[@data-cat=\"Top_Ten_En_Scroll\"]')\n",
    "lap_name = [((i.text).split(\"\\n\"))[1] for i in l_name]\n",
    "\n",
    "specs_info = driver.find_elements_by_xpath('//div[@class=\"Specs-Wrap\"]')\n",
    "l = [i.text.split(\"\\n\") for i in specs_info]\n",
    "\n",
    "#for OS info\n",
    "OS = [i[1] for i in l]\n",
    "\n",
    "#for Display info\n",
    "Display = [i[3] for i in l]\n",
    "\n",
    "#for processor info\n",
    "Processor = [i[5] for i in l]\n",
    "\n",
    "#for memory info\n",
    "Memory = [i[7] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_info = driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]')\n",
    "specs_info = [(i.text).split(\"\\n\") for i in specs_info]\n",
    "\n",
    "#for weght info\n",
    "weight = [(i[5].split(\":\"))[1].strip() for i in specs_info]\n",
    "\n",
    "#for Dimension info\n",
    "Dimension = [(i[6].split(\":\"))[1].strip() for i in specs_info]\n",
    "\n",
    "#for Graphics preocessor\n",
    "g_proc = [(i[7].split(\":\"))[1].strip() for i in specs_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for scrappng the description\n",
    "descrption = driver.find_elements_by_xpath('//div[@class=\"Section-center\"]')\n",
    "dec = [(i.text.split(\"\\n\"))[0] for i in descrption]\n",
    "len(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AVAILABLE',\n",
       " 'AVAILABLE',\n",
       " 'AVAILABLE',\n",
       " '-',\n",
       " 'AVAILABLE',\n",
       " 'AVAILABLE',\n",
       " 'AVAILABLE',\n",
       " 'AVAILABLE',\n",
       " 'AVAILABLE',\n",
       " 'AVAILABLE']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for availabilty\n",
    "avai = driver.find_elements_by_xpath('//p[@style=\"margin: 0px 0 0 0;font-size: 13px;position: relative;font-weight: 400;line-height: 19px;width: 89px;\"]')\n",
    "avai = [i.text for i in avai]\n",
    "avai.insert(3,\"-\")\n",
    "avai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for price\n",
    "price = driver.find_elements_by_xpath('//table[@class=\"table\"]')\n",
    "price = [((i.text.split(\"\\n\"))[0].split(\"₹\"))[1].strip() for i in price]\n",
    "price.insert(3,\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop name</th>\n",
       "      <th>OS</th>\n",
       "      <th>Display</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Memory</th>\n",
       "      <th>weight</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Graphics Processor</th>\n",
       "      <th>Availablity</th>\n",
       "      <th>Price</th>\n",
       "      <th>Descripton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACER NITRO 5</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>AMD RYZEN 9 OCTA CORE | 2.4 GHZ</td>\n",
       "      <td>1 TB HDD/16 GBGB DDR4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>363.4 x 255 x 23.9</td>\n",
       "      <td>NVIDIA GeForce RTX 3070</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>129,990</td>\n",
       "      <td>Possibly the best value-for-money gaming lapto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI STEALTH 15M</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>INTEL CORE I7 11TH GEN - 11375H | NA</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>358.3 x 248 x 16.15</td>\n",
       "      <td>NVIDIA GeForce RTX 3060</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>134,990</td>\n",
       "      <td>If you’re looking for a powerful gaming laptop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (2560 X 1440)</td>\n",
       "      <td>AMD RYZEN 9 OCTA CORE - 5900HX | 3.3 GHZ</td>\n",
       "      <td>2 TB SSD/32 GBGB DDR4</td>\n",
       "      <td>2.30</td>\n",
       "      <td>354 x 259 x 22.6</td>\n",
       "      <td>NVIDIA GeForce RTX 3080</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>268,990</td>\n",
       "      <td>If you want possibly the best performance poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALIENWARE AREA 51M R2</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>17.3\" (1920 X 1080)</td>\n",
       "      <td>10TH GEN INTEL® CORE™ I7-10700 | 2.90 GHZ</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>27.65 x 402.6 x 319.14</td>\n",
       "      <td>Intel® UHD Graphics 630</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>If you want the absolute best when it comes to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALIENWARE M15 R3</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (3840 X 2160)</td>\n",
       "      <td>10TH GEN INTEL® CORE™ I9-10980HK | NA</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>AVAILABLE</td>\n",
       "      <td>319,990</td>\n",
       "      <td>If the Area 51M is a bit too much for you then...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Laptop name               OS              Display  \\\n",
       "0            ACER NITRO 5       WINDOWS 10  15.6\" (1920 X 1080)   \n",
       "1         MSI STEALTH 15M       WINDOWS 10  15.6\" (1920 X 1080)   \n",
       "2  ASUS ROG STRIX SCAR 15       WINDOWS 10  15.6\" (2560 X 1440)   \n",
       "3   ALIENWARE AREA 51M R2  WINDOWS 10 HOME  17.3\" (1920 X 1080)   \n",
       "4        ALIENWARE M15 R3  WINDOWS 10 HOME  15.6\" (3840 X 2160)   \n",
       "\n",
       "                                   Processor                 Memory weight  \\\n",
       "0            AMD RYZEN 9 OCTA CORE | 2.4 GHZ  1 TB HDD/16 GBGB DDR4    2.4   \n",
       "1       INTEL CORE I7 11TH GEN - 11375H | NA  1 TB SSD/16 GBGB DDR4    1.7   \n",
       "2   AMD RYZEN 9 OCTA CORE - 5900HX | 3.3 GHZ  2 TB SSD/32 GBGB DDR4   2.30   \n",
       "3  10TH GEN INTEL® CORE™ I7-10700 | 2.90 GHZ  1 TB SSD/16 GBGB DDR4    4.1   \n",
       "4      10TH GEN INTEL® CORE™ I9-10980HK | NA  1 TB SSD/16 GBGB DDR4     NA   \n",
       "\n",
       "                Dimension       Graphics Processor Availablity    Price  \\\n",
       "0      363.4 x 255 x 23.9  NVIDIA GeForce RTX 3070   AVAILABLE  129,990   \n",
       "1     358.3 x 248 x 16.15  NVIDIA GeForce RTX 3060   AVAILABLE  134,990   \n",
       "2        354 x 259 x 22.6  NVIDIA GeForce RTX 3080   AVAILABLE  268,990   \n",
       "3  27.65 x 402.6 x 319.14  Intel® UHD Graphics 630           -        -   \n",
       "4                      NA                       NA   AVAILABLE  319,990   \n",
       "\n",
       "                                          Descripton  \n",
       "0  Possibly the best value-for-money gaming lapto...  \n",
       "1  If you’re looking for a powerful gaming laptop...  \n",
       "2  If you want possibly the best performance poss...  \n",
       "3  If you want the absolute best when it comes to...  \n",
       "4  If the Area 51M is a bit too much for you then...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Laptop name\":lap_name,\"OS\":OS,\"Display\":Display,\"Processor\":Processor,\"Memory\":Memory,\n",
    "                  \"weight\":weight,\"Dimension\":Dimension,\"Graphics Processor\":g_proc,\"Availablity\":avai,\n",
    "                  \"Price\":price,\"Descripton\":dec})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Top_gaming_laptops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.8) Write a python program to scrape the details for all billionaires  from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://www.forbes.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for option button to chose Billionaires from all over the world,we can get a web element for that using an absolute xpath\n",
    "option_button = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[1]/button[1]\")\n",
    "\n",
    "option_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_option_button = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "\n",
    "sub_option_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "billionaires_option_button = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "\n",
    "billionaires_option_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the all required lists\n",
    "rank,name,n_worth,age,citizen,source,industry = [],[],[],[],[],[],[]\n",
    "\n",
    "t = True\n",
    "\n",
    "#Scrapping the details\n",
    "while t == True:\n",
    "    details = driver.find_elements_by_xpath('//div[@role=\"row\"]')\n",
    "    d = [i.text.split(\"\\n\") for i in details][1:]\n",
    "\n",
    "    for i in d:\n",
    "        rank.append(i[0])\n",
    "        name.append(i[1])\n",
    "        n_worth.append(i[2])\n",
    "        age.append(i[3])\n",
    "        citizen.append(i[4])\n",
    "        source.append(i[5])\n",
    "        industry.append(i[6])\n",
    "    \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath(\"/html/body/div[1]/div[1]/div/div/div[3]/div[2]/div[2]/div[2]/div[27]/div[7]/div[2]/button[2]/div/i\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        t = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Net worth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>$177 B</td>\n",
       "      <td>57</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>$151 B</td>\n",
       "      <td>49</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>$150 B</td>\n",
       "      <td>72</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>$124 B</td>\n",
       "      <td>65</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>$97 B</td>\n",
       "      <td>36</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Daniel Yong Zhang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>49</td>\n",
       "      <td>China</td>\n",
       "      <td>e-commerce</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhang Yuqiang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>65</td>\n",
       "      <td>China</td>\n",
       "      <td>Fiberglass</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhao Meiguang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>gold mining</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhong Naixiong</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>conglomerate</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhou Wei family</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>54</td>\n",
       "      <td>China</td>\n",
       "      <td>Software</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2755 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rank                      Name Net worth Age    Citizenship  \\\n",
       "0        1.                Jeff Bezos    $177 B  57  United States   \n",
       "1        2.                 Elon Musk    $151 B  49  United States   \n",
       "2        3.  Bernard Arnault & family    $150 B  72         France   \n",
       "3        4.                Bill Gates    $124 B  65  United States   \n",
       "4        5.           Mark Zuckerberg     $97 B  36  United States   \n",
       "...     ...                       ...       ...  ..            ...   \n",
       "2750  2674.         Daniel Yong Zhang      $1 B  49          China   \n",
       "2751  2674.             Zhang Yuqiang      $1 B  65          China   \n",
       "2752  2674.             Zhao Meiguang      $1 B  58          China   \n",
       "2753  2674.            Zhong Naixiong      $1 B  58          China   \n",
       "2754  2674.           Zhou Wei family      $1 B  54          China   \n",
       "\n",
       "             Source          Industry  \n",
       "0            Amazon        Technology  \n",
       "1     Tesla, SpaceX        Automotive  \n",
       "2              LVMH  Fashion & Retail  \n",
       "3         Microsoft        Technology  \n",
       "4          Facebook        Technology  \n",
       "...             ...               ...  \n",
       "2750     e-commerce        Technology  \n",
       "2751     Fiberglass     Manufacturing  \n",
       "2752    gold mining   Metals & Mining  \n",
       "2753   conglomerate       Diversified  \n",
       "2754       Software        Technology  \n",
       "\n",
       "[2755 rows x 7 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Rank\":rank,\"Name\":name,\"Net worth\":n_worth,\"Age\":age,\"Citizenship\":citizen,\"Source\":source,\"Industry\":industry})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"All Billionaires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.9)Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Listen to my NEW song \"One Thing Right\" with Kane Brown ▶ https://youtu.be/O6RyKbcpBfw\\n\\nWatch Cooking with Marshmello HERE ▶ https://www.youtube.com/playlist?list=PLcYK4PlHbZQtXROf5fnrr4dO4ruWiv7ts',\n",
       " 'We are in love with this music 😍',\n",
       " \"Don't worry you aren't alone listening to this masterpiece 💟\",\n",
       " \"It's crazy how many people come back to this everyday.This song is legendary♪\",\n",
       " \"It's July 2021 let's see how many people still listening to this masterpiece\",\n",
       " 'Tantos recuerdos que me trae esta canción en 2017 oírla una y otra vez son simplemente \\n✨mágicos✨',\n",
       " 'Músicas como éstas merecen estár en la historia ❤️😍',\n",
       " 'Hello :D\\nالعرب هنا لنشر السلام والحب نحن نحب الجميع ❤\\n*We Are Love All ❤',\n",
       " \"No escuchaba esta canción hace 2 años y medio algo así y la primera vez que le escuché fue en mi país cerca de mis amigos y conocidos hoy después de tanto tiempo la eschucho de nuevo pero esta vez acompañada de una dulce y amarga nostalgia :')\",\n",
       " 'Honestly I think this is one of my favourite songs ever, keep its up with the great vids Marshmello LOVE EM',\n",
       " 'This song never gets old.  I REMEMBERED SINGING THIS EVERYDAY IN THE RADIO...NOSTALGIC.',\n",
       " 'i always love ur music keep up the awsome work!!',\n",
       " \"it's been 4 years since this golden age passed, but I'm here again listening to Mello's good times, good times\",\n",
       " 'Marshmello:  One of the only artists that can make a love song without any lyrics.',\n",
       " 'That positive vibes and goosebumps on body really make this song more legendary..❤️',\n",
       " 'Listening to music, I remember old days when corona was only a drink.',\n",
       " 'I love this song it never gets old ;)',\n",
       " 'can you believe it’s been 4 years now!i just wish that we could still be in that year',\n",
       " \"Whenever I listen up this track, my teen memories come alive! To be honest, Marshmello's some tracks took me to the EDM community. Anyway, he still catches my attention!\",\n",
       " 'Hope you enjoy the Summer video even though it is still winter ;)\\n\\nWow! 150 MILLION plays 💗💗💗 Thank you #Mellogang',\n",
       " 'this will keep me motivated until summer, thank you so much for this fine piece of work!',\n",
       " 'La escucho y me devuelve al pasado ❤',\n",
       " 'Nostalgia demais vei até de 2014 até 2018 foi bom demais',\n",
       " 'Es bellísima 😍 hermosa canción 💕',\n",
       " \"Why do I feel like everything's gonna be alright when I listen to Marshmello?\",\n",
       " 'This song is such a big inspiration to me, sometimes when I feel discouraged I listen to this song and it motivates me to continue creating better music and chasing my dreams. I hope one day my music will be recognised and successful as well, It would a dream come true.\\n\\nTo everyone reading this, God bless you and your family. May you achieve what you aspire and may all your dreams come true as well.',\n",
       " 'Stop asking who is listening to this song in 2021, we never stop listening.',\n",
       " 'Ya tres años de está hermosa canción ❤️😍',\n",
       " 'E simplesmente o melhor!!!',\n",
       " 'A día de hoy me sigue gustando, buena marshmello 😎👏',\n",
       " 'Amooo de mais',\n",
       " 'I like it alot, gets me in the mood for the day',\n",
       " 'Marshmello Makes me so happy no matter if the song is bad or good keep it up man',\n",
       " 'Adoro escutar essa música pela manhã',\n",
       " \"It's been 4 years since this golden age passed, but I'm here again listening to Mello's good times, good times\",\n",
       " 'Essa música é muito show cara 🤙🏽👍🏽👏🏽👏🏽👏🏽👏🏽👏🏽 .',\n",
       " 'Eu amo verão,i love summer',\n",
       " 'Muito boua !',\n",
       " 'La musica electrónica es súper cool.Gracias  Marshmello 👍',\n",
       " 'Recuerdo perfectamente la primera vez que la escuché 😢. Extraño a la personita que me la hizo escuchar ❄️.',\n",
       " 'love this!!!!!!!!!!!',\n",
       " 'This song is so relaxing😍',\n",
       " 'Love the music Mello! Music is a way to forget or remember it all! Can’t wait to share some of my music with ya!',\n",
       " 'The song I listened to 5 years ago will never leave my mind♪',\n",
       " 'Oh my God those summer vibes 😍',\n",
       " 'Absolutely amazing song!',\n",
       " 'Marsmello es el que alegra mis días',\n",
       " 'Temazo ♥️',\n",
       " 'When I first heard his one of the famous track alone, I literally loved 😍 it after hearing it the first time. \\nThen for few months, I was listening all of his tracks and after that I got inspired in making edm music. \\nI would say because of marshmello, I started my journey as a edm music producer. \\nThank you for inspiring me.',\n",
       " 'This one is a masterpiece!👑❤',\n",
       " 'I remember listening to this during the summer. Great Times',\n",
       " '2:35 I love that tranquil melody ❤️',\n",
       " 'she fell in love with a marshmallow?\\nBetter love story than twilight.',\n",
       " 'I love this music ❤️',\n",
       " 'Just loved it💞💞',\n",
       " \"For 3 years I tried to listen to this song exactly on the last day of school, but I forgot. Well now I didn't. The fight is over. This felt awesome.\",\n",
       " 'Still listening to it even after 4 years 🥰',\n",
       " 'Legend song , still enjoy every beat 💓',\n",
       " 'this man has been prepared for covid this whole time',\n",
       " \"Trop bien ta musique j'adore 😊\",\n",
       " 'This made my heart melt❤️',\n",
       " 'Amo suas músicas',\n",
       " 'Marshmallow Eletrônica é Demais 🎶🎵🎶',\n",
       " 'Essa música é perfeita',\n",
       " \"Me da nostalgia volver a escuchar esta canción :')\",\n",
       " 'Me encanta❤',\n",
       " \"The only person in the whole industry who sends tonnes of positive vibes through each and every song! that's why we love you soo muchh MELLO!!!\",\n",
       " 'Hoy cumple 4 años esta maravillosa canción saludos marshmello\\nViva México!!!!!',\n",
       " 'Eu amo verão,e tbm amo o DJ marshmallow amo música eletrônica',\n",
       " 'We love all of your videos and your song!🥰🥰',\n",
       " 'We need more electronic music @marshmello in 2021 🔥🔥🔥',\n",
       " 'I listened to this song for the first time today!! 😍',\n",
       " 'your music calms me down and is great :) keep up the good work!!',\n",
       " 'This style This melody AWESOME 😍',\n",
       " 'I love your music 🎶 ❤️',\n",
       " 'Gosto das suas músicas 😻😻😻😻😻',\n",
       " 'This song is best to me every time 😍💕 🔥',\n",
       " \"It feels something better ❤️ which I can't express\",\n",
       " 'In love with the music ❤❤',\n",
       " 'Love the music!',\n",
       " 'cade os brasileiros! #Marshmello é top demais',\n",
       " 'Thank u marshmello it brings smile whenever i listen to this track',\n",
       " \"Came back after a few years, this make's me happy thinking back on the old times\",\n",
       " 'This song fixes my mood everytime',\n",
       " 'AMO TUS CANCIONES MARSHMELLO!!!!!',\n",
       " 'This song will never get old!!!\\nMany people come back listening to this masterpiece in every seconds. \\n: )',\n",
       " 'Posted on my birthday hehe 💙 :)',\n",
       " 'Keep up the good work marshmallo, ;)',\n",
       " '4 years ago I remember when this came out and I loved it',\n",
       " 'Enero del 2022 y aun seguimos aquí escuchando esta legendaria!!!',\n",
       " 'parabens mashmello 50 milhoes',\n",
       " 'Amo assisti😘',\n",
       " 'I can listen to this all day long.',\n",
       " 'Vários anos se passam e marshmallow continua surfando no hype. BRASILEIROS. 🇧🇷 . Deixa o like 👍🏽',\n",
       " 'Adorei amo muito os seus vídeos ❤💖👍',\n",
       " '너무 좋아요 ♥♥',\n",
       " 'Love the beat!!!',\n",
       " 'You can feel it even without the lyrics on it ☺️',\n",
       " 'This is....DOPE! ♥ \\nI love your songs ♥',\n",
       " 'Quem Está Ouvindo Em 2021 Está Aqui Sucesso Total Show😎💥👏🏻✌🏻',\n",
       " 'There is no one better than marshmello😘😘',\n",
       " 'Buena musica me gusta esta cancion amo la musica de marshmello❤🤩🙏',\n",
       " 'Often I listen to Happier and cry, then I used to listen Summer to get my emotions up. great works. Love from India :)',\n",
       " '\"Verão\" é bom ver meu idioma em lugares assim',\n",
       " 'I hope marshmello never stop doing music and videos like this',\n",
       " \"Whenever I listen to this music, I feel like the summer is a really awesome weather but when the temperature reaches 45+ celsius then I tell myself that it's not awesome lol\",\n",
       " 'His music is so wonderful',\n",
       " 'I still remember this song ❤️❤️',\n",
       " 'I think all of marshmallows songs are his life put together and keep being amazing marshmallow',\n",
       " 'this song brings back so many memories of me n my old self,itfeels good just thinking of it.\\nTHANK YOU',\n",
       " 'MUITO TOP',\n",
       " 'Doesn’t say a word and he gets the ladies. True player for real!',\n",
       " 'Amo a Marshmallow sobre todo porque se preocupa de los ignorantes que no hablamos inglés :v',\n",
       " 'Es algo increíble me encanta',\n",
       " 'Me gusta 😍',\n",
       " 'Marshmallow you are the best! I listen you every day,then I become very happy',\n",
       " \"Trop bien celle-ci j'adore c'est ma préférée🤩🤩🤩😊😊😊\",\n",
       " 'After 4 years still this songs is on my phone till now❤️❤️❤️❤️',\n",
       " 'idk why but this vid made me happy lmao. love the song too',\n",
       " 'I feel inspired by your work',\n",
       " 'Welcome 2019!',\n",
       " '4 years, 4 months later... still fire',\n",
       " 'Zajebista muza',\n",
       " 'Красиво 👍🏻',\n",
       " 'La mejor música electrónica 🎧🎧',\n",
       " \"Though it is 'Summer' I'm playing it in 'Winter' 😂 ♥️♥️✨\",\n",
       " 'SUMMER! Im already playing this one daily! i hope marshmello will collab with sky parox, or alan walker',\n",
       " \"For those who don't know this song is my favorite because the lyrics are very touching.\",\n",
       " 'Still a bop tho. ♥️😩',\n",
       " 'As vezes a gente acha que não vai superar um grande amor que vc teve por tão pouco tempo, mas depois de dois anos de sofrimento hoje venho aqui dizer que não sinto mais o que sentia antes.',\n",
       " 'Nostalgia 💚',\n",
       " '\"\"\"It\\'s crazy how many people come back to this everyday.This song is legendary. This Song Is Never Getting Old\"\"\" ♡❣♡',\n",
       " 'One of the best musics that i have heard..continue like that ! you deserve everything because you done everything well! Good luck Marshmello ♡♡ ;) i love you so much! * I WANT YOU TO ANSWER ME :( *',\n",
       " 'I just came back. to this song after 4. years and still how I. remember its in the top 20 in my opinion.',\n",
       " 'I just love the way she opens her hair.😍',\n",
       " 'Que vídeo más hermoso!!!😍😍  Muy especial 💞💞 Marshmello eres el mejor, gracias bro por tus hermosas canciones 💖💖',\n",
       " \"Listening in Winters But This song feels me Like it's Really Summer 🔥\",\n",
       " 'Summer vibes ✨',\n",
       " \"it's summer in Argentina.. and this song is so popular and cool here :)\\nwe love Marshmello !\",\n",
       " 'This song never gets old',\n",
       " 'i love it...\\n\\nNICE WORKS BRO!!!',\n",
       " 'What a masterpiece',\n",
       " 'One of the best song in electronic music 🤩',\n",
       " 'I still listen to all of his music💙',\n",
       " \"Обожаю этот супер трек! Thank's!\",\n",
       " '2021 😁 4 years and I still listen to this music and Alone moving on🇹🇭',\n",
       " 'Marshmello is a  good guy 🙂😉',\n",
       " 'Felicidades por tus 50 millones marsh',\n",
       " 'This song is As much as people coming to watch more and more now a days ❤️❤️❤️',\n",
       " 'LA MAYORÍA DE LA MÚSICA DE  MARSHMELLO ME GUSTA QUIEN LA ESCUCHA TODAVÍA',\n",
       " 'The feel good music 😍',\n",
       " 'La mejor música la amo',\n",
       " \"IT'S SUMMER!! 👊🎆🎧\",\n",
       " 'Bro,\\nYou have became the drugs for the people who are full of stress.\\nLove for you😍❤',\n",
       " 'Minha preferida 💗 saudades de uma pessoa 😢',\n",
       " \"I'm leaving this comment here in hope that whenever someone likes it, I'll be reminded of this masterpiece!\",\n",
       " 'First song I ever heard from Marshmello 😊',\n",
       " 'Awesome tune 😍❤️✌️',\n",
       " 'Demuestren que son fanáticos de marshmello y denle like a sus vídeos 👍',\n",
       " 'muy bueno tu música marshmello',\n",
       " '¿Alguien que hable en Español que también ame a Marshmello?\\n✖️.  ✖️\\n    ➖',\n",
       " 'me encantó esta rola',\n",
       " 'sou brasileiro mais gosto muito de vc mello',\n",
       " 'amei',\n",
       " 'Mis favoritos juntos',\n",
       " 'The first music of Marshmello I have listened.., and I then immediately I became his fan❤❤',\n",
       " \"Aww, Marshmello got a girlfriend now he's not alone anymore\",\n",
       " 'Eu sou do Brasil eu não intendo sua voz mais você deve entender a minha voz \\n\\n\\n\\n\\nSó sei que sua música é umas das melhores músicas boas que eu já escutei top 10 parabéns 😘😊',\n",
       " \"It's crazy how many people come back to this everyday.This song is legendary.\",\n",
       " 'Background music really wow, amazing 🔥🔥',\n",
       " 'musics like this give me a reason to live',\n",
       " \"This song is too nostalgic tbh, it's like picking up on old memories\",\n",
       " 'WHO THINKS MARSHMELLO IS BEAST👍',\n",
       " 'I love that everything he has is shaped like marshmello.... pillows , shoes , ......\\nMust be fun 😆😆😆',\n",
       " 'Som Irado',\n",
       " 'This song reminds me of my childhood every time I listen to it. Growing up sucks lol',\n",
       " 'It’s a amazing song',\n",
       " 'The bass drop is legendary wow! Saludos desde Puerto Rico 🇵🇷',\n",
       " 'My favorite marshmello and i listen this song every day 2018 😍😍😍',\n",
       " 'Gorgeous ❤️❤️❤️❤️',\n",
       " 'I love marshamello Music🎤🎼🎹🎶....... Lesson from Music...... Love from India🇮🇳🇮🇳🇮🇳 💕😘💕💕💕',\n",
       " '2022 still addicted to this 🎶',\n",
       " 'my favorite song of his cause it is a true summer song where you have no problems and life is banging 🔥🔥',\n",
       " 'Everything ❣️',\n",
       " 'Lovely ❤❤',\n",
       " 'this song brings back memories',\n",
       " 'Damn! Lele Pons got some blends of Margot Robbie..',\n",
       " 'Legend . I love MEXICO ❤🇲🇽',\n",
       " \"Ilove Lele I'm so proud of her she was recognized now in music she had talent\",\n",
       " 'Literally Heart Touching Music , memories become alive of my childhood at my village playing with my siblings during summer vacations. Heart Touching ( if you just listen song and dont see video )',\n",
       " '\"It\\'s crazy how many people come back to this everyday. This song is legendary!\"♡',\n",
       " '\"\"\"It\\'s crazy how many people come back to this everyday.This song is legendary. It\\'s So Addictive Also\"\"\" ♡♡♡',\n",
       " 'Who else got a good feeling when the boss started smiling',\n",
       " '4 yrs and I still look to him to hope to a better future',\n",
       " 'Still on my top list of fav song 😘',\n",
       " 'What a masterpiece ,love this song',\n",
       " 'Man so nostalgic... I remember passing 8th grade and listening to this song on the ride home with some friends',\n",
       " 'Matshmello has a very positive vibe. I like it👍',\n",
       " \"I'm in love ❤️ ❤️ ❤️ ❤️ ❤️ ❤️ ❤️ ❤️ ❤️\\nLoved it\",\n",
       " 'if you see this comment, just know that I’m proud of you. even though we haven’t met, i think you’re such a special person :)🖤',\n",
       " 'I want Living High to be released officially it sounds so good',\n",
       " \"In the middle of song, I come to know that this song doesn't have lyrics 🤯🤯🤯\\nThat's the power of music.....❤️\",\n",
       " 'I thought I was the only one here at the end of 2021 but I see that I am not the only one to whom this music gives joy to the heart',\n",
       " 'Awesome vast incredible ❤️',\n",
       " 'Great music',\n",
       " 'Nostagia🔥',\n",
       " \"I dont know if you guys knew this but Lele's actually Latina.\",\n",
       " 'Marshmellow has an Incredible talent of producing Music.\\nAnd that CLOWN took my heart ♥ 😌',\n",
       " 'No puede ser hace 4 años no sabía quien era lele pons ahora quedé, igual alto temazo',\n",
       " 'Que buenos tiempos 2017',\n",
       " 'love this song',\n",
       " \"Recuerdos...  :'D\",\n",
       " '4 April 2018 , still love this',\n",
       " 'Me gusta la música de Allan Walker, pues, esta es una de las mejores del mundo',\n",
       " 'love the music',\n",
       " 'The videos are not always the best but musics are always good🙂',\n",
       " 'Love the music',\n",
       " \"En esos tiempos mi vida era genial:')\",\n",
       " 'If Alan Walker and Marshmello make a song together I can die in peace',\n",
       " \"This song reminds me of the times when we didn't have to worry about social distancing and wearing masks. We'd live normally and have fun. So when this is all over, y'all already KNOW we finna be going to a roller skating place and be playing this song my G's\",\n",
       " 'música fodaaa❤🎶\\nalgum BR ai ?',\n",
       " 'The love  can him change anyone 😍',\n",
       " 'me encantaaa',\n",
       " \"It's crazy how many legends keep coming back to this song every day, this song is really legendary ...........\",\n",
       " 'Marshmallow man rocks on every level 👌',\n",
       " 'Perfecto nwn',\n",
       " 'The ultimate summer anthem for me are 1. marshmello - summer, 2. calvin harris - summer, 3. sky parox - beach blessings.. how about yours?',\n",
       " 'Low key this song is relaxing',\n",
       " 'This has always lifted my spirit.',\n",
       " 'Nice Music 😍',\n",
       " \"I don't know how to express myself when I watch Marshmello videos\",\n",
       " 'Perfect ✌❤',\n",
       " 'It is so cold here in Europe and the last thing I can think of is summer.',\n",
       " 'Every music video is a unique story.',\n",
       " 'Still loving this song!',\n",
       " 'Still a masterpiece even summer holidays are finished😥',\n",
       " 'Just tune amazing ❤️🎉',\n",
       " 'The nostalgia makes me want to shed a tear',\n",
       " 'Lele and Marshmello?\\n\\n\\n\\n\\n\\nWell, I liked, commented, subscribed. WHAT ELSE CAN I DO TO SHOW MY LOVE FOR THIS MUSIC VIDEO MAN?!',\n",
       " 'Simplemente eres el mejor',\n",
       " \"Marshmello, you're the god of music.\",\n",
       " 'Amazing😍',\n",
       " \"Let's be honest I'm  never tired of  listening to this song.\",\n",
       " 'Its crazy how many people come back here everyday to listen to this masterpiece. Its iconic',\n",
       " 'Boa de mais ❤\\nAguem do Brasil 🇧🇷 2021',\n",
       " 'Que nostalgia viaje del 2021 al 2019 con esta música',\n",
       " 'Cadê os BR fã do Marshmello',\n",
       " 'Músicas tops demais',\n",
       " 'Eres Genial ♥',\n",
       " \"Bro it's been 5 years it's 2022 this is still masterpiece 💞🥴\",\n",
       " 'Sempre que você escutar uma música eu estarei lá!',\n",
       " 'It’s Never Been The Same Anymore :)',\n",
       " 'I love you marshmello!! 🤘👌💯😍😘😊😎😄😆😜',\n",
       " 'I have never seen this song but just now love it',\n",
       " 'Nunca olvidare tus canciones NUNCA¡¡¡ MARSHMELLO¡¡¡¡¡¡¡',\n",
       " 'El video estuvo muy bueno. Me gusto',\n",
       " 'Just listen this song today. Such a beautiful without even singing cannot believe 😄☺😁😘',\n",
       " '2022 we still banging this',\n",
       " 'i love your music is my favorite',\n",
       " \"Dear parents, just because your child is smiling at their phone, doesn't mean they have boyfriend or girlfriend, they are just watching this masterpiece.\"]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "l = []\n",
    "def ScrapComment(url):\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    prev_h = 0\n",
    "    while True:\n",
    "        height = driver.execute_script(\"\"\"\n",
    "                function getActualHeight() {\n",
    "                    return Math.max(\n",
    "                        Math.max(document.body.scrollHeight, document.documentElement.scrollHeight),\n",
    "                        Math.max(document.body.offsetHeight, document.documentElement.offsetHeight),\n",
    "                        Math.max(document.body.clientHeight, document.documentElement.clientHeight)\n",
    "                    );\n",
    "                }\n",
    "                return getActualHeight();\n",
    "            \"\"\")\n",
    "        driver.execute_script(f\"window.scrollTo({prev_h},{prev_h + 200})\")\n",
    "        time.sleep(1)\n",
    "        prev_h +=200  \n",
    "        if prev_h >= height:\n",
    "            break\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    #title_text_div = soup.select_one('#container h1')\n",
    "    #title = title_text_div and title_text_div.text\n",
    "    comment_div = soup.select(\"#content #content-text\")\n",
    "    comment_list = [x.text for x in comment_div]\n",
    "    \n",
    "    return comment_list\n",
    "    \n",
    "ScrapComment(\"https://www.youtube.com/watch?v=2vMH8lITTCE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.10) Write a python program to scrape a data for all available Hostels https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews,overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since it takes too much time in extracting data from all pages along with description.\n",
    "#### So,I will extract data only for First page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://www.hostelworld.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searchng for a web element of search bar\n",
    "search_bar = driver.find_element_by_xpath(\"/html/body/div[3]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[1]/div/div/div/input\")\n",
    "\n",
    "#Now,entering the \"London\" as location\n",
    "search_bar.send_keys(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectng the london(England)\n",
    "\n",
    "select_bar = driver.find_element_by_xpath(\"/html/body/div[3]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]\")\n",
    "\n",
    "select_bar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing a lets go button\n",
    "\n",
    "lets_go = driver.find_element_by_xpath(\"/html/body/div[3]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[5]/button\")\n",
    "\n",
    "lets_go.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for hostel names\n",
    "name = driver.find_elements_by_xpath('//h2[@class=\"title title-6\"]')\n",
    "name = [i.text for i in name]\n",
    "\n",
    "#for distance from city centre\n",
    "dist_centre = driver.find_elements_by_xpath('//span[@class=\"description\"]')\n",
    "dist_centre = [i.text for i in dist_centre]\n",
    "\n",
    "feedback_info = driver.find_elements_by_xpath('//div[@class=\"rating rating-summary-container big\"]')\n",
    "#for rating\n",
    "rating = [(i.text.split(\"\\n\"))[0] for i in feedback_info]\n",
    "\n",
    "#for total reviews\n",
    "t_reviews = [((i.text.split(\"\\n\"))[2].split())[0] for i in feedback_info]\n",
    "\n",
    "#for overall review\n",
    "o_review = [(i.text.split(\"\\n\"))[1] for i in feedback_info]\n",
    "\n",
    "\n",
    "price = driver.find_elements_by_xpath('//div[@class=\"price-col\"]')\n",
    "#for dorm \n",
    "price_d = []\n",
    "for i in range(1,len(price),2):\n",
    "    if len((price[i].text).split(\"\\n\")) > 1:\n",
    "        price_d.append((((price[i].text).split(\"\\n\")[1]).split(\" \"))[-1])\n",
    "    else:\n",
    "        price_d.append((price[i].text).split(\"\\n\")[0])\n",
    "\n",
    "#for private  \n",
    "price_p = []\n",
    "for i in range(0,len(price),2):\n",
    "    if len((price[i].text).split(\"\\n\")) > 1:\n",
    "        price_p.append((((price[i].text).split(\"\\n\")[1]).split(\" \"))[-1])\n",
    "    else:\n",
    "        price_p.append((price[i].text).split(\"\\n\")[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc,fac = [],[]\n",
    "\n",
    "#finding all the items in page\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "result = soup.find_all('div',{'class':'property-card'})\n",
    "\n",
    "\n",
    "# finding web element of item by class\n",
    "for e_item in range(len(result)):\n",
    "    item = result[e_item]\n",
    "    atag = item.h2.a\n",
    "\n",
    "    try:\n",
    "        url = \"https://www.hostelworld.com\" + atag.get('href')\n",
    "        driver.get(url)\n",
    "#         show_more = driver.find_element_by_xpath('/html/body/div[3]/div/div/div[1]/section/div[6]/div/div[2]/div/div/a')\n",
    "#         show_more.click()\n",
    "        \n",
    "        soup_n = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        result_n = soup_n.find('div',{'class':'content collapse-content'})\n",
    "        desc.append(result_n.text)\n",
    "        \n",
    "        result_n1 = soup_n.find('ul',{'class':'facilities'})\n",
    "        fac.append(result_n1.text)\n",
    "        \n",
    "        driver.back()\n",
    "        \n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using regex to remove all unwanted spaces\n",
    "import re\n",
    "desc = [sub.replace(\"\\n\", \"\").strip() for sub in desc]\n",
    "desc1 = [sub.replace(\"\\r\", \"\").strip() for sub in desc]\n",
    "\n",
    "fac = [sub.replace(\"\\r\", \"\").strip() for sub in fac]\n",
    "fac1 = [sub.replace(\"\\n\", \"\").strip() for sub in fac]\n",
    "fac1 = [re.sub(' +', ' ', test_str) for test_str in fac1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hostel name</th>\n",
       "      <th>Distance from city centre</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Total reviews</th>\n",
       "      <th>Overall Review</th>\n",
       "      <th>Privates from price</th>\n",
       "      <th>dorms from price</th>\n",
       "      <th>Facilities</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mornington Camden</td>\n",
       "      <td>Hostel - 4.1km from city centre</td>\n",
       "      <td>8.3</td>\n",
       "      <td>44</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>No Privates Available</td>\n",
       "      <td>Rs3260</td>\n",
       "      <td>Linen Included Free City Maps Towels Included ...</td>\n",
       "      <td>Mornington Camden Hostel is a high class spaci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smart Russell Square Hostel</td>\n",
       "      <td>Hostel - 2.6km from city centre</td>\n",
       "      <td>6.7</td>\n",
       "      <td>9567</td>\n",
       "      <td>Good</td>\n",
       "      <td>Rs3467</td>\n",
       "      <td>Rs1117</td>\n",
       "      <td>Linen Included Free City Maps Free WiFi Free I...</td>\n",
       "      <td>Nestled in the heart of London, walking distan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smart Camden Inn Hostel</td>\n",
       "      <td>Hostel - 4.4km from city centre</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2740</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>No Privates Available</td>\n",
       "      <td>Rs1193</td>\n",
       "      <td>Linen Included Free City Maps Free WiFi</td>\n",
       "      <td>Located in the middle of Camden Town, we’re th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selina Camden</td>\n",
       "      <td>Hostel - 5.5km from city centre</td>\n",
       "      <td>9.2</td>\n",
       "      <td>15</td>\n",
       "      <td>Superb</td>\n",
       "      <td>Rs11325</td>\n",
       "      <td>Rs2888</td>\n",
       "      <td>Linen Included Towels Included Free WiFi</td>\n",
       "      <td>Among underground music venues, innovative mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queen Elizabeth Chelsea</td>\n",
       "      <td>Hostel - 5.7km from city centre</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3214</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Rs3382</td>\n",
       "      <td>Rs1403</td>\n",
       "      <td>Linen Included Free City Maps Free WiFi Free I...</td>\n",
       "      <td>PLEASE NOTE WE ONLY ACCEPT GUESTS FROM THE AGE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Hostel name        Distance from city centre Rating  \\\n",
       "0            Mornington Camden  Hostel - 4.1km from city centre    8.3   \n",
       "1  Smart Russell Square Hostel  Hostel - 2.6km from city centre    6.7   \n",
       "2      Smart Camden Inn Hostel  Hostel - 4.4km from city centre    8.5   \n",
       "3                Selina Camden  Hostel - 5.5km from city centre    9.2   \n",
       "4      Queen Elizabeth Chelsea  Hostel - 5.7km from city centre    7.5   \n",
       "\n",
       "  Total reviews Overall Review    Privates from price dorms from price  \\\n",
       "0            44       Fabulous  No Privates Available           Rs3260   \n",
       "1          9567           Good                 Rs3467           Rs1117   \n",
       "2          2740       Fabulous  No Privates Available           Rs1193   \n",
       "3            15         Superb                Rs11325           Rs2888   \n",
       "4          3214      Very Good                 Rs3382           Rs1403   \n",
       "\n",
       "                                          Facilities  \\\n",
       "0  Linen Included Free City Maps Towels Included ...   \n",
       "1  Linen Included Free City Maps Free WiFi Free I...   \n",
       "2            Linen Included Free City Maps Free WiFi   \n",
       "3           Linen Included Towels Included Free WiFi   \n",
       "4  Linen Included Free City Maps Free WiFi Free I...   \n",
       "\n",
       "                                         Description  \n",
       "0  Mornington Camden Hostel is a high class spaci...  \n",
       "1  Nestled in the heart of London, walking distan...  \n",
       "2  Located in the middle of Camden Town, we’re th...  \n",
       "3  Among underground music venues, innovative mus...  \n",
       "4  PLEASE NOTE WE ONLY ACCEPT GUESTS FROM THE AGE...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Hostel name\":name,\"Distance from city centre\":dist_centre,\"Rating\":rating,\"Total reviews\":t_reviews,\n",
    "                  \"Overall Review\":o_review,\"Privates from price\":price_p,\"dorms from price\":price_d,\n",
    "                  \"Facilities\":fac1,\"Description\":desc1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Hostel world Details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.5) Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on https://www.google.com/maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://www.google.com/maps'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangalore\n"
     ]
    }
   ],
   "source": [
    "#finding the web element for search bar to send keys\n",
    "name = driver.find_element_by_xpath('/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/form/div/div[3]/div/input[1]')\n",
    "\n",
    "city_name = input()\n",
    "name.send_keys(city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to search for entered city\n",
    "name = driver.find_element_by_xpath('/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/div[1]/button')\n",
    "\n",
    "name.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can get a latitude and longitude of city using a Nominatim library\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geobased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude of Bangalore is :- 12.9767936\n",
      "longtitude of Bangalore is:- 77.590082\n"
     ]
    }
   ],
   "source": [
    "loc = geolocator.geocode(city_name)\n",
    "print((\"latitude of {0} is :- {2}\\nlongtitude of {1} is:- {3}\").format(city_name,city_name,loc.latitude,loc.longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.3)Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site url\n",
    "url = 'https://images.google.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Extracting the \"Cars\"  images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars\n"
     ]
    }
   ],
   "source": [
    "#finding the web element for search bar to send keys\n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input')\n",
    "\n",
    "city_name = input()\n",
    "name.send_keys(city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the web element for search button \n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button')\n",
    "\n",
    "name.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the thumbnail images\n",
    "\n",
    "image_url = []\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "def soup_to_find(soup):\n",
    "    result_n = soup.find_all('div',{'class':'isv-r PNCib MSM1fd BUooTd'})\n",
    "    result_n = [i for i in result_n][:10]\n",
    "    for i in result_n:\n",
    "        i = i.img\n",
    "        image_url.append(i[\"src\"])\n",
    "soup_to_find(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download an images we can use following library\n",
    "\n",
    "import urllib.request\n",
    "for i in range(len(image_url)):\n",
    "    urllib.request.urlretrieve(image_url[i], \"Pictures/Cars/cars_\"+str(i)+\".jpg\")\n",
    "    \n",
    "cars_img_url = image_url.copy()\n",
    "# going back to main page\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)  Extracting the \"Fruits\"  images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruits\n"
     ]
    }
   ],
   "source": [
    "#finding the web element for search bar to send keys\n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input')\n",
    "\n",
    "city_name = input()\n",
    "name.send_keys(city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the web element for search button \n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button')\n",
    "\n",
    "name.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the thumbnail images\n",
    "\n",
    "image_url = []\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "soup_to_find(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download an images we can use following library\n",
    "i = 0\n",
    "import urllib.request\n",
    "for i in range(len(image_url)):\n",
    "    urllib.request.urlretrieve(image_url[i], \"Pictures/Fruits/fruits_\"+str(i)+\".jpg\")\n",
    "\n",
    "fruits_img_url = image_url.copy()\n",
    "# going back to main page\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)  Extracting the \"Machine learning\"  images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning\n"
     ]
    }
   ],
   "source": [
    "#finding the web element for search bar to send keys\n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input')\n",
    "\n",
    "city_name = input()\n",
    "name.send_keys(city_name)\n",
    "\n",
    "#finding the web element for search button \n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button')\n",
    "\n",
    "name.click()\n",
    "\n",
    "#scrapping the thumbnail images\n",
    "image_url = []\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "soup_to_find(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download an images we can use following library\n",
    "i = 0\n",
    "import urllib.request\n",
    "for i in range(len(image_url)):\n",
    "    urllib.request.urlretrieve(image_url[i], \"Pictures/ML/ML_\"+str(i)+\".jpg\")\n",
    "\n",
    "ML_img_url = image_url.copy()\n",
    "\n",
    "# going back to main page\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)  Extracting the \"guitar\"  images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guitar\n"
     ]
    }
   ],
   "source": [
    "#finding the web element for search bar to send keys\n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input')\n",
    "\n",
    "city_name = input()\n",
    "name.send_keys(city_name)\n",
    "\n",
    "#finding the web element for search button \n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button')\n",
    "\n",
    "name.click()\n",
    "\n",
    "#scrapping the thumbnail images\n",
    "image_url = []\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "soup_to_find(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download an images we can use following library\n",
    "i = 0\n",
    "import urllib.request\n",
    "for i in range(len(image_url)):\n",
    "    urllib.request.urlretrieve(image_url[i], \"Pictures/Guitars/Guitar_\"+str(i)+\".jpg\")\n",
    "\n",
    "guitar_img_url = image_url.copy()\n",
    "# going back to main page\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)  Extracting the \"Cakes\"  images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cakes\n"
     ]
    }
   ],
   "source": [
    "#finding the web element for search bar to send keys\n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input')\n",
    "\n",
    "city_name = input()\n",
    "name.send_keys(city_name)\n",
    "\n",
    "#finding the web element for search button \n",
    "name = driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button')\n",
    "\n",
    "name.click()\n",
    "\n",
    "#scrapping the thumbnail images\n",
    "image_url = []\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "soup_to_find(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download an images we can use following library\n",
    "i = 0\n",
    "import urllib.request\n",
    "for i in range(len(image_url)):\n",
    "    urllib.request.urlretrieve(image_url[i], \"Pictures/Cakes/cake_\"+str(i)+\".jpg\")\n",
    "\n",
    "cake_img_url = image_url.copy()\n",
    "# going back to main page\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the dataframe of all catagories image urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cars image url</th>\n",
       "      <th>Fruits image url</th>\n",
       "      <th>ML image url</th>\n",
       "      <th>Guitars image url</th>\n",
       "      <th>Cakes image url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "      <td>data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Cars image url  \\\n",
       "0  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "1  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "2  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "3  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "4  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "5  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "6  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "7  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "8  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "9  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "\n",
       "                                    Fruits image url  \\\n",
       "0  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "1  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "2  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "3  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "4  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "5  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "6  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "7  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "8  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "9  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "\n",
       "                                        ML image url  \\\n",
       "0  data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...   \n",
       "1  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "2  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "3  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "4  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "5  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "6  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "7  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "8  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "9  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "\n",
       "                                   Guitars image url  \\\n",
       "0  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "1  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "2  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "3  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "4  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "5  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "6  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "7  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "8  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "9  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...   \n",
       "\n",
       "                                     Cakes image url  \n",
       "0  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "1  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "2  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "3  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "4  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "5  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "6  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "7  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "8  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  \n",
       "9  data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQA...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Cars image url\":cars_img_url,\"Fruits image url\":fruits_img_url,\"ML image url\":ML_img_url,\n",
    "                  \"Guitars image url\":guitar_img_url,\"Cakes image url\":cake_img_url})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
